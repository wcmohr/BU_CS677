import pandas as pd
import numpy as np
from function_wrappers import *
from functions import *
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

from functions import hyperparameter_grid

















gme = pd.read_csv('./gme.csv')
gme.drop(['Date', 'Month', 'Day','Week_Number','High', 'Low', 'Close', 'Volume', 
         'Short_MA', 'Long_MA'], inplace=True, axis=1)


head_tail(gme,2)











gme_wr = trade_from_close_close(gme)


gme_labels = pd.read_csv('./gme_labels.csv',header=None)[0]
gme_wr.loc[:,'Label'] = "NoLabel"
# apply labels collected by visual inspection for last two years
gme_wr.loc[gme_wr.shape[0] - gme_labels.shape[0]:,'Label'] = gme_labels.values


head_tail(gme_wr)








d='dog'
print(d)





# kwargs = {'measure':'Return'}
polynomial_accuracy(gme,y=2020,w=12,d=2,test=1,measure='Adj Close')








hp_adj_train = hyperparameter_grid(gme, 2020, range(5,13), range(1,4), 0, 'Adj Close')
hp_ret_train = hyperparameter_grid(gme, 2020, range(5,13), range(1,4), 0, 'Return')
hp_adj_test = hyperparameter_grid(gme, 2020, range(5,13), range(1,4), 1, 'Adj Close')
hp_ret_test = hyperparameter_grid(gme, 2020, range(5,13), range(1,4), 1, 'Return')
hp_frames = [hp_adj_train,hp_ret_train,hp_adj_test,hp_ret_test]

frame_names=["Adj_Close_Tr","Return_Tr","Adj_Close_Te","Return_Te"]
frame_features=['d','w','accuracy']
mi = pd.MultiIndex.from_tuples(zip([name*3 for name in frame_names],['d','w','accuracy']*4
                                  ))
hp_accuracies = pd.concat([hp_frame['accuracy'] for hp_frame in hp_frames],
                          axis=1,keys=frame_names,names=['Measure_EvalSet'])
hp_accuracies.index=pd.MultiIndex.from_tuples(
    [(d,w) for d in range(1,4) for w in range (5,13)],names=('d','w'))

hp_accuracies_stacked = pd.DataFrame(hp_accuracies.stack()).reset_index()

hp_accuracies_stacked = hp_accuracies_stacked.rename(columns={0:"Accuracy"})

hp_accuracies_stacked['EvalSet'] = hp_accuracies_stacked['Measure_EvalSet'].apply(
    lambda x: 'Train' if x[-2:] == 'Tr' else 'Test')

hp_accuracies_stacked['Measure'] = hp_accuracies_stacked['Measure_EvalSet'].apply(lambda x: x[:-3])

hp_accuracies_stacked.head()


g = sns.FacetGrid(data=hp_accuracies_stacked,
              row = 'Measure', 
              col='EvalSet',
              hue='d'
             )
g.map(plt.plot,'w','Accuracy').add_legend()
plt.show()


best_w = hp_adj_train.iloc[hp_adj_train.groupby('d')["accuracy"].idxmax().values,:]
best_w_dict={d:w for d,w in zip(best_w['d'],best_w['w'])}
best_w_dict


cm_dict = {}
for k,v in best_w_dict.items():
    
    res = polynomial_accuracy(gme,2020,w=best_w_dict[k],d=k,test=1,measure="Adj Close")[-1]
    
    cm = confusion_matrix(res['true_labels'],res['predicted_labels'])
    cm_dict[(k,v)] = cm


cm_dict


for cm_k,cm_v in cm_dict.items():
    display = ConfusionMatrixDisplay(cm_v)
    display.plot(cmap='Greens')
    display.ax_.set_title(f'd = {cm_k[0]}, w = {cm_k[1]}')
    plt.show()





hp_adj_test.iloc[hp_adj_test.groupby('d')["accuracy"].idxmax().values,:]


hp_ret_train.iloc[hp_ret_train.groupby('d')["accuracy"].idxmax().values,:]


hp_ret_test.iloc[hp_ret_test.groupby('d')["accuracy"].idxmax().values,:]





best_w_dict


gme


polynomial_accuracy(gme,y=2020,w=5,d=2,test=1,measure='Adj Close')


predictions={}
for d,w in best_w_dict.items():
    predictions[(d,w)] = polynomial_accuracy(gme,2020,d=d,w=w,test=1,measure='Adj Close')[-1]['predicted_labels']


predictions


gme_wr = trade_from_close_close(gme).query('Year == 2021')
returns = {}
for d_w,p in predictions.items():
    returns[d_w] = trade_labels(gme_wr,2021,p)


returns

















gme[gme['Year']==2020].index[0]


X_transform(gme,y=2020,w=5,d=1,measure='Adj Close')


gme["Adj Close"]



